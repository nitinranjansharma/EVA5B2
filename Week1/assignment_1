Q What are Channels and Kernels (according to EVA)?

Images can be represented as a combination of multiple layers, with each layers have similar feature but mostly different among each other.
Like a colour image can be made from a combination of RED,BLUE and GREEN colours. 
We have RED, GREEN, BLUE channels and that comprise an image, similarly, anything which is of a similar pattern can comprise a channel, like if words are like images aphabets would symbolise
channels.
Kernels are feature extractors, an image can be a combination of various features. Any image can be thought to be created from multiple strokes of straight
lines and hence an edge can be a feature. On a braoder sense, kernels are used to determine what featres are important from the context of solving
the problem being under the loss function of the CNN architecture and extract it. Mathematially its a two dimentional matrix (mostly 3*3/2*2) of randomly initialized numbers. 
In the context of EVA, any image can be viewed as a combination of different edges and gradients. These 3*3 matrix/kernels traverse on the image and try 
to identify different features.

Q Why should we (nearly) always use 3x3 kernels?
Kernels are feature extractors, and since we are dealing with highly complex image as an information source, a scalable solution with optimal number
of trainable parameters are required.
Firstly, Kernels needs to extract features, so if we assume that features are line then we need an odd number to get the line perfectly in the middle. 
Secondly a 3*3 extracts features using less number of parameters, example 2 3*3 can use 18 parameters
but 1 5*5 will use 25 parameters, thus 3*3 kernel serves the purpose of extracting granular information. 

Q How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)
Layer 1 - 199*199 conv 3*3 > 197*197
Layer 2 - 197*197 conv 3*3 > 195*195
Layer 3 - 195*195 conv 3*3 > 193*193
Layer 4 - 193*193 conv 3*3 > 191*191
Layer 5 - 191*191 conv 3*3 > 189*189
Layer 6 - 189*189 conv 3*3 > 187*187
Layer 7 - 187*187 conv 3*3 > 185*185
Layer 8 - 185*185 conv 3*3 > 183*183
Layer 9 - 183*183 conv 3*3 > 181*181
Layer 10 - 181*181 conv 3*3 > 179*179
Layer 11 - 179*179 conv 3*3 > 177*177
Layer 12 - 177*177 conv 3*3 > 175*175
Layer 13 - 175*175 conv 3*3 > 173*173
Layer 14 - 173*173 conv 3*3 > 171*171
Layer 15 - 171*171 conv 3*3 > 169*169
Layer 16 - 169*169 conv 3*3 > 167*167
Layer 17 - 167*167 conv 3*3 > 165*165
Layer 18 - 165*165 conv 3*3 > 163*163
Layer 19 - 163*163 conv 3*3 > 161*161
Layer 20 - 161*161 conv 3*3 > 159*159
Layer 21 - 159*159 conv 3*3 > 157*157
Layer 22 - 157*157 conv 3*3 > 155*155
Layer 23 - 155*155 conv 3*3 > 153*153
Layer 24 - 153*153 conv 3*3 > 151*151
Layer 25 - 151*151 conv 3*3 > 149*149
Layer 26 - 149*149 conv 3*3 > 147*147
Layer 27 - 147*147 conv 3*3 > 145*145
Layer 28 - 145*145 conv 3*3 > 143*143
Layer 29 - 143*143 conv 3*3 > 141*141
Layer 30 - 141*141 conv 3*3 > 139*139
Layer 31 - 139*139 conv 3*3 > 137*137
Layer 32 - 137*137 conv 3*3 > 135*135
Layer 33 - 135*135 conv 3*3 > 133*133
Layer 34 - 133*133 conv 3*3 > 131*131
Layer 35 - 131*131 conv 3*3 > 129*129
Layer 36 - 129*129 conv 3*3 > 127*127
Layer 37 - 127*127 conv 3*3 > 125*125
Layer 38 - 125*125 conv 3*3 > 123*123
Layer 39 - 123*123 conv 3*3 > 121*121
Layer 40 - 121*121 conv 3*3 > 119*119
Layer 41 - 119*119 conv 3*3 > 117*117
Layer 42 - 117*117 conv 3*3 > 115*115
Layer 43 - 115*115 conv 3*3 > 113*113
Layer 44 - 113*113 conv 3*3 > 111*111
Layer 45 - 111*111 conv 3*3 > 109*109
Layer 46 - 109*109 conv 3*3 > 107*107
Layer 47 - 107*107 conv 3*3 > 105*105
Layer 48 - 105*105 conv 3*3 > 103*103
Layer 49 - 103*103 conv 3*3 > 101*101
Layer 50 - 101*101 conv 3*3 > 99*99
Layer 51 - 99*99 conv 3*3 > 97*97
Layer 52 - 97*97 conv 3*3 > 95*95
Layer 53 - 95*95 conv 3*3 > 93*93
Layer 54 - 93*93 conv 3*3 > 91*91
Layer 55 - 91*91 conv 3*3 > 89*89
Layer 56 - 89*89 conv 3*3 > 87*87
Layer 57 - 87*87 conv 3*3 > 85*85
Layer 58 - 85*85 conv 3*3 > 83*83
Layer 59 - 83*83 conv 3*3 > 81*81
Layer 60 - 81*81 conv 3*3 > 79*79
Layer 61 - 79*79 conv 3*3 > 77*77
Layer 62 - 77*77 conv 3*3 > 75*75
Layer 63 - 75*75 conv 3*3 > 73*73
Layer 64 - 73*73 conv 3*3 > 71*71
Layer 65 - 71*71 conv 3*3 > 69*69
Layer 66 - 69*69 conv 3*3 > 67*67
Layer 67 - 67*67 conv 3*3 > 65*65
Layer 68 - 65*65 conv 3*3 > 63*63
Layer 69 - 63*63 conv 3*3 > 61*61
Layer 70 - 61*61 conv 3*3 > 59*59
Layer 71 - 59*59 conv 3*3 > 57*57
Layer 72 - 57*57 conv 3*3 > 55*55
Layer 73 - 55*55 conv 3*3 > 53*53
Layer 74 - 53*53 conv 3*3 > 51*51
Layer 75 - 51*51 conv 3*3 > 49*49
Layer 76 - 49*49 conv 3*3 > 47*47
Layer 77 - 47*47 conv 3*3 > 45*45
Layer 78 - 45*45 conv 3*3 > 43*43
Layer 79 - 43*43 conv 3*3 > 41*41
Layer 80 - 41*41 conv 3*3 > 39*39
Layer 81 - 39*39 conv 3*3 > 37*37
Layer 82 - 37*37 conv 3*3 > 35*35
Layer 83 - 35*35 conv 3*3 > 33*33
Layer 84 - 33*33 conv 3*3 > 31*31
Layer 85 - 31*31 conv 3*3 > 29*29
Layer 86 - 29*29 conv 3*3 > 27*27
Layer 87 - 27*27 conv 3*3 > 25*25
Layer 88 - 25*25 conv 3*3 > 23*23
Layer 89 - 23*23 conv 3*3 > 21*21
Layer 90 - 21*21 conv 3*3 > 19*19
Layer 91 - 19*19 conv 3*3 > 17*17
Layer 92 - 17*17 conv 3*3 > 15*15
Layer 93 - 15*15 conv 3*3 > 13*13
Layer 94 - 13*13 conv 3*3 > 11*11
Layer 95 - 11*11 conv 3*3 > 9*9
Layer 96 - 9*9 conv 3*3 > 7*7
Layer 97 - 7*7 conv 3*3 > 5*5
Layer 98 - 5*5 conv 3*3 > 3*3
Layer 99 - 3*3 conv 3*3 > 1*1



Q How are kernels initialized? 

kernels are initialised by taking random values from a distribution. Ideally if we assume a normal distribution the paths covered by the backpropagations
to converge will be lesser as compared to initialising by any fixed number. If we initialize kernels with a fixed value there might be possibilities when there 
is a huge difference beteween initalzed (initialized at 1) and final value (actual value 1000) the model takes a lot of time to convere as the update is comparetively small. 
For the cases where we have a gap between the final value of variables and both are initialized as same, there is a possibility one variable value might ossicilate during 
back propagation as the other value hasn't converge yet.

Q What happens during the training of a DNN?

A deep neural network is a broader term for any neural network which has a bit of higher complexities like existing 2 or more hidden layers.
Layers are basically set of neurons which has its own weights and biases associated with it. 
A training of such architecture suggests that the network learns to adjust the weights and biases for each neurons such that it can understand the data 
distribution which the architecture is trying to undertstand throuh a designated loss function. So loss function provides for a goal to a DNN for it to
learn and undertand the problem. The goal is generally solved using a technique called back propagation.
1st iteration - The inputs followed by the neurons with all the biases and weghts get the data and produce an output.
the output gets compared with the actual output and then a direction is created through a system which determines which direction the updates
of weights of neuron be such that the loss function will give a lesser error. This iterations are followed multiple times untill a state is reached where
we can say that it has conclusively reached a minima.